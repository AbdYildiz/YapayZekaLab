{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGWFwrOjEN3s",
        "outputId": "fd6aaf43-021f-419c-d2ad-495f73c23b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "disleri temizle\n"
          ]
        }
      ],
      "source": [
        "# Uygulama 4.4\n",
        "from random import choice\n",
        "from experta import *\n",
        "\n",
        "class Doctor(Fact):\n",
        "    \"\"\"   \"\"\"\n",
        "    pass\n",
        "\n",
        "class goToDoctor(KnowledgeEngine):\n",
        "    # dis fircalarken dis eti kanamasi olursa\n",
        "    @Rule(Doctor(issue=\"short bleeding\"))\n",
        "    def tissueBleeding(self):\n",
        "        print(\"dis hastaligi vardir ve dis hekimine basvur\")\n",
        "\n",
        "    # dis fircalarken uzun sureli dis eti kanamasi olursa\n",
        "    @Rule(Doctor(issue=\"too much bleeding\"))\n",
        "    def tooMuchTissueBleeding(self):\n",
        "        print(\"diseti cekilmesi vardir ve dis hekimine basvur\")\n",
        "\n",
        "    # eger dis eti cekilmesi var ve dis koku gorunuyorsa\n",
        "    @Rule(Doctor(issue=\"teeth root visible\"))\n",
        "    def isTeethRootVisible(self):\n",
        "        print(\"dolgu yaptir\")\n",
        "\n",
        "    # diste yiyecek ve iceceklerden olusan renk degisimi varsa\n",
        "    @Rule(Doctor(issue=\"color change\"))\n",
        "    def cleanTeeth(self):\n",
        "        print(\"disleri temizle\")\n",
        "\n",
        "    # yeni dis cikarirken morarma gorunuyorsa\n",
        "    @Rule(Doctor(issue=\"it's purple\"))\n",
        "    def isItPurple(self):\n",
        "        print(\"dis hekimine basvur\")\n",
        "\n",
        "    # diste agri yapmayan curuk varsa\n",
        "    @Rule(Doctor(issue=\"rotten teeth\"))\n",
        "    def fillUp(self):\n",
        "        print(\"dolgu yaptir\")\n",
        "\n",
        "    # disteki curuk ileri derecedeyse\n",
        "    @Rule(Doctor(issue=\"severe rotten teeth\"))\n",
        "    def canalCure(self):\n",
        "        print(\"kanal tedavisi ve dolgu yaptir\")\n",
        "\n",
        "\n",
        "uzman = goToDoctor()\n",
        "\n",
        "uzman.reset()\n",
        "uzman.declare(Doctor(issue=choice(['short bleeding', 'too much bleeding', 'teeth root visible',\n",
        "                                   'color change', 'it\\'s purple', 'rotten teeth', 'severe rotten teeth'])))\n",
        "uzman.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uygulama 5.4\n",
        "# Yaprak siniflandirmasi (1DESA)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# test ve egitim verilerinin okunmasi\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Siniflarin belirlenmesi ve etiketlenmesi\n",
        "label_encoder = LabelEncoder().fit(train.species)\n",
        "labels = label_encoder.transform(train.species)\n",
        "classes = list(label_encoder.classes_)\n",
        "\n",
        "# verilerin hazirlanmasi, ozellik ve sinif sayisinin belirlenmesi\n",
        "train = train.drop([\"id\", \"species\"], axis=1)\n",
        "test = test.drop([\"id\"], axis=1)\n",
        "nb_features = 192\n",
        "nb_classes = len(classes)\n",
        "\n",
        "# egitim verisindeki verilen standartlastirilirmasi\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler().fit(train.values)\n",
        "train = scaler.transform(train.values)\n",
        "\n",
        "# egitim verisinin egitim ve dogrulama icin ayarlanmasi\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(train, labels, test_size=0.1)\n",
        "\n",
        "# etiketlerin kategorilerinin belirlenmesi\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_valid = to_categorical(y_valid)\n",
        "\n",
        "# giris verilerinin boyutlarinin ayarlanmasi\n",
        "x_train = np.array(x_train).reshape(891, 192, 1)\n",
        "x_valid = np.array(x_valid).reshape(99, 192, 1)\n",
        "\n",
        "# 1DESA modelinin olusturulmasi\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv1D, Dropout, MaxPooling1D, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(512, 1, input_shape=(nb_features, 1)))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Conv1D(256, 1))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2048, activation=\"relu\"))\n",
        "model.add(Dense(1024, activation=\"relu\"))\n",
        "model.add(Dense(nb_classes, activation=\"softmax\"))\n",
        "model.summary()\n",
        "\n",
        "# agin derlenmesi\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# modelin egitilmesi\n",
        "model.fit(x_train, y_train, epochs=10, validation_data=(x_valid, y_valid))\n",
        "\n",
        "# ortalama degerlerin gosterilmesi\n",
        "print(\"Ortalama Egitim Kaybi: \", np.mean(model.history.history[\"loss\"]))\n",
        "print(\"Ortalama Egitim Basarimi: \", np.mean(model.history.history[\"accuracy\"]))\n",
        "print(\"Ortalama Dogrulama Kaybi: \", np.mean(model.history.history[\"val_loss\"]))\n",
        "print(\"Ortalama Dogrulama Basarim: \", np.mean(model.history.history[\"val_accuracy\"]))\n",
        "\n",
        "dtc = DecisionTreeClassifier()\n",
        "y_pred = dtc.predict(x_valid)\n",
        "score = f1_score(y_pred, y_valid)\n",
        "print(\"F1 score: {:.2f}\".format(score))"
      ],
      "metadata": {
        "id": "thT814-UFI8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uygulama 6.4\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# test ve egitim verilerinin okunmasi\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Siniflarin belirlenmesi ve etiketlenmesi\n",
        "label_encoder = LabelEncoder().fit(train.species)\n",
        "labels = label_encoder.transform(train.species)\n",
        "classes = list(label_encoder.classes_)\n",
        "\n",
        "train = train.drop([\"id\"], axis=1)\n",
        "test = test.drop([\"id\"], axis=1)\n",
        "nb_features = 192\n",
        "nb_classes = len(classes)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler().fit(train.values)\n",
        "train = scaler.transform(train.values)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(train, labels, test_size=0.1)\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_valid = to_categorical(y_valid)\n",
        "\n",
        "x_train = np.array(x_train).reshape(891, 192, 1)\n",
        "x_valid = np.array(x_valid).reshape(99, 192, 1)\n",
        "\n",
        "IMAGE_WIDTH = 128\n",
        "IMAGE_HEIGHT = 128\n",
        "IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "IMAGE_CHANNELS = 3\n",
        "\n",
        "filenames = os.listdir(\"CNN/train\")\n",
        "categories = []\n",
        "for filename in filenames:\n",
        "\tcategory = filename.split('.')[0]\n",
        "\tif category == 'tumor':  \n",
        "\t\tcategories.append(1)\n",
        "\telse:\n",
        "\t\tcategories.append(0)  \n",
        "df = pd.DataFrame({'filename': filenames, 'category': categories})\n",
        "\n",
        "df['category'].value_counts().plot.bar()\n",
        "\n",
        "# modelin olusturulmasi\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(nb_classes, activation='softmax'))  # nb_classes tane sinif\n",
        "model.summary()\n",
        "\n",
        "#  modelin derlenmesi\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "# verinin hazirlanmasi\n",
        "train_df, validate_df = train_test_split(df, test_size= 0.2)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "validate_df = validate_df.reset_index(drop=True)\n",
        "# kategorilere bakilmasi\n",
        "train_df[\"category\"].value_counts().plot.bar()\n",
        "# egitim ve dogrulama verisinin hazirlanmasi\n",
        "total_train = train_df.shape[0]\n",
        "total_validate = validate_df.shape[0]\n",
        "batch_size = 15\n",
        "\n",
        "#  egitim verilerinin cogaltilmasi\n",
        "train_datagen = ImageDataGenerator(\n",
        "\trotation_range = 15,\n",
        "\trescale = 1./255,\n",
        "\tshear_range = 0.1,\n",
        "\tzoom_range = 0.2,\n",
        "\thorizontal_flip = True,\n",
        "\twidth_shift_range = 0.1,\n",
        "\theight_shift_range = 0.1\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "\ttrain_df,\n",
        "\t\"images\",\n",
        "\tx_col = 'filename',\n",
        "\ty_col = 'category',\n",
        "\ttarget_size=IMAGE_SIZE,\n",
        "\tclass_mode='categorical',\n",
        "\tbatch_size=batch_size\n",
        ")\n",
        "\n",
        "# dogrulama verilerinin cogaltilmasi\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_generator =validation_datagen.flow_from_dataframe(\n",
        "\tvalidate_df,\n",
        "\t\"images\",\n",
        "\tx_col = 'filename',\n",
        "\ty_col = 'category',\n",
        "\ttarget_size=IMAGE_SIZE,\n",
        "\tclass_mode='categorical',\n",
        "\tbatch_size=batch_size\n",
        ")\n",
        "\n",
        "# modelin egitilmesi\n",
        "epochs = 15  # 10\n",
        "history = model.fit(\n",
        "\ttrain_generator,\n",
        "\tepochs=epochs,\n",
        "\tvalidation_data=validation_generator,\n",
        "\tvalidation_steps=total_validate//batch_size,\n",
        "\tsteps_per_epoch=total_train//batch_size\n",
        ")\n",
        "\n",
        "# olusturulan modelin kaydedilmesi\n",
        "model.save_weights(\"model1.h5\")\n",
        "# tahmin isleminin yapilmasi\n",
        "predict = model.predict_generator(test_generator, steps=np.ceil(nb_samples/batch_size))"
      ],
      "metadata": {
        "id": "SE7-glVYGxzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uygulama 7.4\n",
        "import numpy as np  # linear algebra\n",
        "import pandas as pd  # CSV file\n",
        "import scipy.io.wavfile as sci_wav  # Open wav files\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "ROOT_DIR = \"CNN Voice/cats_dogs/\"\n",
        "CSV_PATH = \"CNN Voice/train_test_split.csv\"\n",
        "\n",
        "\n",
        "def read_wav_files(wav_files):\n",
        "\tif not isinstance(wav_files, list):\n",
        "\t\twav_files = [wav_files]\n",
        "\treturn [sci_wav.read(ROOT_DIR + f)[1] for f in wav_files]\n",
        "\n",
        "\n",
        "def get_trunk(_X, idx, sample_len, rand_offset=False):\n",
        "\trandint = np.random.randint(10000) if rand_offset is True else 0\n",
        "\tstart_idx = (idx * sample_len + randint) % len(_X)\n",
        "\tend_idx = ((idx + 1) * sample_len + randint) % len(_X)\n",
        "\tif end_idx > start_idx:  # normal case\n",
        "\t\treturn _X[start_idx: end_idx]\n",
        "\telse:\n",
        "\t\treturn np.concatenate((_X[start_idx:], _X[:end_idx]))\n",
        "\n",
        "\n",
        "def get_augmented_trunk(_X, idx, sample_len, added_samples=0):\n",
        "\tX = get_trunk(_X, idx, sample_len)\n",
        "\n",
        "\t# Add other audio of the same class to this sample\n",
        "\tfor _ in range(added_samples):\n",
        "\t\tridx = np.random.randint(len(_X))  # random index\n",
        "\t\tX = X + get_trunk(_X, ridx, sample_len)\n",
        "\n",
        "\t# One might add more processing (like adding noise)\n",
        "\n",
        "\treturn X\n",
        "\n",
        "\n",
        "def dataset_gen(is_train=True, batch_shape=(20, 16000), sample_augmentation=0):\n",
        "\ts_per_batch = batch_shape[0]\n",
        "\ts_len = batch_shape[1]\n",
        "\n",
        "\tX_cat = dataset['train_cat'] if is_train else dataset['test_cat']\n",
        "\tX_dog = dataset['train_dog'] if is_train else dataset['test_dog']\n",
        "\n",
        "\t# Go through all the permutations\n",
        "\ty_batch = np.zeros(s_per_batch)\n",
        "\tX_batch = np.zeros(batch_shape)\n",
        "\t# Random permutations (for X indexes)\n",
        "\tnbatch = int(max(len(X_cat), len(X_cat)) / s_len)\n",
        "\tperms = [list(enumerate([i] * nbatch)) for i in range(2)]\n",
        "\tperms = sum(perms, [])\n",
        "\trandom.shuffle(perms)\n",
        "\n",
        "\n",
        "\n",
        "\twhile len(perms) > s_per_batch:\n",
        "\n",
        "\t\t# Generate a batch\n",
        "\t\tfor bidx in range(s_per_batch):\n",
        "\t\t\tperm, _y = perms.pop()  # Load the permutation\n",
        "\t\t\ty_batch[bidx] = _y\n",
        "\n",
        "\t\t\t# Select wether the sample is a cat or a dog\n",
        "\t\t\t_X = X_cat if _y == 0 else X_dog\n",
        "\n",
        "\t\t\t# Apply the permutation to the good set\n",
        "\t\t\tif is_train:\n",
        "\t\t\t\tX_batch[bidx] = get_augmented_trunk(\n",
        "\t\t\t\t\t_X,\n",
        "\t\t\t\t\tidx=perm,\n",
        "\t\t\t\t\tsample_len=s_len,\n",
        "\t\t\t\t\tadded_samples=sample_augmentation)\n",
        "\t\t\telse:\n",
        "\t\t\t\tX_batch[bidx] = get_trunk(_X, perm, s_len)\n",
        "\n",
        "\t\tyield (X_batch.reshape(s_per_batch, s_len, 1),\n",
        "\t\t       y_batch.reshape(-1, 1))\n",
        "\n",
        "\n",
        "def load_dataset(dataframe):\n",
        "\tdf = dataframe\n",
        "\n",
        "\tdataset = {}\n",
        "\tfor k in ['train_cat', 'train_dog', 'test_cat', 'test_dog']:\n",
        "\t\tv = list(df[k].dropna())\n",
        "\t\tv = read_wav_files(v)\n",
        "\t\tv = np.concatenate(v).astype('float32')\n",
        "\n",
        "\t\t# Compute mean and variance\n",
        "\t\tif k == 'train_cat':\n",
        "\t\t\tdog_std = dog_mean = 0\n",
        "\t\t\tcat_std, cat_mean = v.std(), v.mean()\n",
        "\t\telif k == 'train_dog':\n",
        "\t\t\tdog_std, dog_mean = v.std(), v.mean()\n",
        "\n",
        "\t\t# Mean and variance suppression\n",
        "\t\tstd, mean = (cat_std, cat_mean) if 'cat' in k else (dog_std, dog_mean)\n",
        "\t\tv = (v - mean) / std\n",
        "\t\tdataset[k] = v\n",
        "\n",
        "\t\tprint('loaded {} with {} sec of audio'.format(k, len(v) / 16000))\n",
        "\n",
        "\treturn dataset\n",
        "\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "dataset = load_dataset(df)\n",
        "\n",
        "batch_size = 512\n",
        "num_data_points = 16000\n",
        "n_augment = 10\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(20, 4, strides=2, activation='relu', input_shape=(num_data_points, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(20, 4, strides=2, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Conv1D(40, 4, strides=2, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(40, 4, strides=2, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Conv1D(80, 4, strides=2, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(80, 4, strides=2, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "adam_optimizer = Adam(decay=1e-3)\n",
        "model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "\n",
        "# loop through epoch samples (batchs)\n",
        "for epochs in range(NUM_EPOCHS):\n",
        "\ttrain_gen = dataset_gen(is_train=True, batch_shape=(batch_size, num_data_points), sample_augmentation=n_augment)\n",
        "\tfor batch_x, batch_y in train_gen:\n",
        "\t\thistory = model.fit(batch_x, batch_y, epochs=1, validation_split=0.2)\n",
        "\t\ttrain_loss.extend(history.history['loss'])\n",
        "\t\tval_loss.extend(history.history['val_loss'])\n",
        "\t\ttrain_acc.extend(history.history['accuracy'])\n",
        "\t\tval_acc.extend(history.history['val_accuracy'])\n",
        "\n",
        "fig = plt.figure(figsize=(15, 8))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(train_loss, label='train loss')\n",
        "ax.plot(val_loss, label='val loss', color='green')\n",
        "plt.legend()\n",
        "plt.title('log loss')\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(15, 8))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(train_acc, label='train accuracy')\n",
        "ax.plot(val_loss, label='val accuracy', color='green')\n",
        "plt.legend()\n",
        "plt.title('accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wje-HtiYIQys"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}